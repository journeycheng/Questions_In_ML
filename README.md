# Questions_In_ML

### 1.数据样本类别分布不平衡

类别不平衡（class-imbalance）指分类任务中不同类别的训练样本数目差别很大。假定正类样本较少，反类样本较多。

- 方法一:欠采样（undersampling）

对训练集里的反类样本进行“欠采样”，即除去一些反类样本使得正、反类数目接近，然后再进行学习。

欠采样若随机丢弃反类样本，可能丢失一些重要信息，欠采样的代表性算法是EasyEnsemble，利用集成学习机制，将反类划分为若干个集合供不同学习器使用，这样对每个学习器来看都进行了欠采样，但在全局来看却不会丢失重要信息。

- 方法二:过采样（oversampling）

对训练集里的正类样本进行“过采样”，增加一些正类样本使得正、反类数目接近，然后进行学习。

过采样不能简单地对初始正类样本进行复制采样，否则会导致严重的过拟合；过采样的代表性算法SMOTE是通过对训练集里的正类样本进行插值来产生额外的正类样本。


- 方法三:阈值移动（threshold-moving）

在LR中，预测出的y值与一个阈值进行比较，通常在y>0.5时判别为正类，否则为反类。y实际上表达了正类的可能性。几率y/(1-y)则反映了正类可能性与反类可能性之比值。

阈值设置为0.5，恰表明分类器认为真实正、反类可能性相同，即分类器决策规则为：若 y/(1-y)>1，则预测为正类。

当训练集中正反类的数目不同时，另Mp表示正类数量，Mn表示反类数量，则观测几率是Mp/Mn。由于我们通常假设训练集是真实样本总体的无偏差采样，因此观测几率就代表了真实几率。于是，只要分类器的预测几率高于观测几率就应判为正类，即

若y/(1-y) > Mp/Mn，则预测为正类。

分类器是基于y/(1-y)>1这个式子进行决策的，因此，需要对预测值进行调整：

y'/(1-y') = y/(1-y)*Mn/Mp

\frac{y^{'}}{1-y^{'}} = \frac{y}{1-y}*\frac{m^-}{m^+}


### 2.决策树中的连续值处理

由于连续属性的可取值数目不再有限，因此，不能直接根据连续属性的可取值对节点进行划分。此时，连续属性离散化技术可派上用场。

最简单的策略是采用二分法对连续属性值进行处理。

给定样本集D和连续属性，假定a在D上出现了n个不同的连续值，将这n个值从小到大排序。考察包含n－1个元素的候选划分点集合Ta = {(a(i)+ a(i+1))/2, 1<=i<=n-1}.分别以不同的候选划分点对数据集进行划分，计算相应的信息增益。从中选择增益最大的候选划分点。

### 3.决策树中的缺失值处理
- 如何在属性值缺失的情况下进行划分属性选择？


问题相当于在有缺失值的情况下，计算根据某一属性划分数据集的信息增益。

将数据集根据某一属性a是否缺失进行划分，其中有数据的数据集占总数据的p(a_val)= M_val/ M。按照正常的方式计算无缺失值部分数据集的信息增益Gain_val。总的信息增益为：p(a_val)\*Gain_val

- 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？

对于属性值缺失，按照以样本分布的概率进入到下一层。
